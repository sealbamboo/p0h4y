{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:43:37.592976Z",
     "start_time": "2019-05-17T05:43:32.220874Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "\n",
    "# Folder path\n",
    "folder = './run_data/'\n",
    "folder_traing = './training/'\n",
    "dest_file = './final/final_content.csv'\n",
    "\n",
    "# Declare some variables\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Model\n",
    "# %run -i ./python-code/nlp.py\n",
    "!python ./python-code/nlp.py\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:34:37.445686Z",
     "start_time": "2019-05-17T05:34:30.618081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 25737 files\n",
      "Reading files in total of 25737\n"
     ]
    }
   ],
   "source": [
    "# Scan through provided folder.\n",
    "entries = os.listdir(folder_traing)\n",
    "print(\"Found: {} files\".format(len(entries)))\n",
    "\n",
    "# Read file and push to list\\n\",\n",
    "contexts = []\n",
    "# list_text.append([f.read() with open(file,'r') as f for file in entries\"]\n",
    "\n",
    "for entry in entries:\n",
    "    file = folder_traing + entry\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.read()\n",
    "        contexts.append(data)\n",
    "#         print(\"====================\\n\")\n",
    "print(\"Reading files in total of {}\".format(len(contexts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:34:37.469392Z",
     "start_time": "2019-05-17T05:34:37.454950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training & testing\n",
    "X_train = contexts[:15442]\n",
    "X_test = contexts[15443:22000]\n",
    "X_spare = contexts[22001::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:43:57.601678Z",
     "start_time": "2019-05-17T05:43:57.566461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'nlp' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/DSi6+/Share/p0h4y/ipynb/python-code/nlp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(test_nlp._expand_contractions(full_text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# test_norm = test_nlp.normalize()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'nlp' object is not callable"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for doc in X_train:\n",
    "    test = nlp(doc, tokenizer,stopword_list)\n",
    "    \n",
    "    corpus.append(test.normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
