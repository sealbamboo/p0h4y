import pandas as pd
import numpy as np

from bs4 import BeautifulSoup
from scrapy.selector import Selector

import re, requests, csv
import lxml

# Regex
regex_url = r"(http(s?):\/\/(www)?[A-Za-z0-9-?\/\.]+)"
regex_hastags = r"(#[A-Za-z0-9]{2,})"

# File url
file_url = '../datasets/foxnewshealth.txt'

# Basic list
csv_header = ['tweetid','date','title']
frame_header = ['tweetid','date','title','what']

# Support scripts
%run -i scripts.py


# -------------------------------------------------
frame = pd.read_table(file_url, sep='|', header= None, names = csv_header, encoding="utf-8")
frame.head()


# -------------------------------------------------
frame.shape


# -------------------------------------------------
frame.loc[:,'title'][0]


# -------------------------------------------------
# Split Url & Text
frame['url'] = frame['title'].map(lambda row: get_url(regex_url, row))
frame['title'] = frame['title'].map(lambda row: split_txt_form_url)
frame.head()


# -------------------------------------------------
# Xpath issues
list_scrapt = ['//div[@class="article-body"]/p/text()',
               '',
               '//*[@id="post-22962"]/header/div[2]/p/span/a/strong/text()'
               ]

# -------------------------------------------------
# Scrapt form web
df_temp = pd.DataFrame()
list_url = frame['url'].values
fetch_contents(list_url, df_temp, list_scrapt)


# -------------------------------------------------
# Run for this jupyter notebook
title_attempt = True
for key,url in enumerate(frame_urls):

    #     url = 'http://bit.ly/VymaIc'
    try:
        # Start the connection
        print(url)
        if len(url):
            if "http://" not in url:
                url = "http://" + url
            res = requests.get(url)

            # Check status header
            if res.status_code == 200:
                print("Header: ", 200)

                # Initialize BeautifulSoup()
                soup = BeautifulSoup(res.content, 'lxml')

                # Get return content as text
                # text_html = res.text

                # Add columns to dataframe
                if title_attempt:
                    df_temp = pd.DataFrame(columns=['author','content','tags'])
                    frame = pd.concat([frame,df_temp])
                    title_attempt = False

                ahref = soup.find('a',{'class':'article_link'})
                if ahref:
                    author = ahref.text            
        #             frame['author'][key] = author[:author.index(",")].lstrip()
                    frame['author'][key] = author.lstrip()
                else:
                    frame['author'] = ''
                frame['content'][key] = soup.find('div',{'class':'article-body'})
            else:
                print("[ERROR]: ", res.status_code)
        else:
            frame['author'][key] = ''
            frame['content'] = ''
        frame['tags'][key] = get_hastags_out(regex_hastags,frame['title'][key])
        frame['title'][key] = stript_hastags(frame['title'][key])
    except (RuntimeError, TypeError, NameError):
        pass



# -------------------------------------------------
frame.head()


# -------------------------------------------------
frame.shape

aws
54.206.67.76
teamview
1 266 389 784

http://…
http://…
http:…
http://t.co/qfm…
http://t.co/srzX5V2N…
http://t.co/ACREVvx…
http…
http://t.co/sjrsMjT…
http://t.co/5GdMnaCd…
http://t.co/Fi3xz…