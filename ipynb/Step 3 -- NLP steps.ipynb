{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:02:41.489129Z",
     "start_time": "2019-05-17T06:02:41.127185Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.utils import simple_preprocess\n",
    "import contractions\n",
    "import unicodedata\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "nlp1 = spacy.load('en_core_web_sm')\n",
    "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, #entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "# stopword_list.remove('no')\n",
    "# stopword_list.remove('not')\n",
    "\n",
    "%run -i ./python-code/nlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:45:26.448007Z",
     "start_time": "2019-05-17T05:45:14.977394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 25737 files\n",
      "Reading files in total of 25737\n"
     ]
    }
   ],
   "source": [
    "folder_traing = './training/'\n",
    "# Scan through provided folder.\n",
    "entries = os.listdir(folder_traing)\n",
    "print(\"Found: {} files\".format(len(entries)))\n",
    "\n",
    "# Read file and push to list\\n\",\n",
    "contexts = []\n",
    "# list_text.append([f.read() with open(file,'r') as f for file in entries\"]\n",
    "\n",
    "for entry in entries:\n",
    "    file = folder_traing + entry\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.read()\n",
    "        contexts.append(data)\n",
    "#         print(\"====================\\n\")\n",
    "print(\"Reading files in total of {}\".format(len(contexts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:00:39.820480Z",
     "start_time": "2019-05-17T05:46:53.044153Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for doc in contexts:\n",
    "    test = nlp(doc, tokenizer,stopword_list)\n",
    "    \n",
    "    corpus.append(test.normalize())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:03:10.556460Z",
     "start_time": "2019-05-17T06:03:10.540733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15 years ago colleagues held party Catford celebrate launch first time bank UK health centre idea make mutual support possible among patients families neighbours wanted see would work UK working time teamed one Rushey Green doctors Richard Byng worried little able patients pills really adequate sustainable solution isolated long term depression range conditions social issues important pharmaceutical ones doctors refer patients think would benefit members stay touch via coordinators members offering requesting help works bit like babysitting circle help someone often something simple lifts gardening phone support earn time need help spend time need companionship lift shops surgery even someone phone time bank sort worked decade half still going strong still making difference peoples lives opposite way charitable endeavour Yes members provide people need something much unusual make people feel needed often people always received come believe around didnt need Feeling useful basic human need time banks provide infrastructure allows people goes lot broader health visiting befriending changing lightbulbs checking people OK theyve discharged hospital menu limited members Rushey Green evaluations unanimous making difference people broadening social networks helping people symptoms especially physical mental issues combined odd health centres copied though example launch project last year senior doctor explained comes radical innovation like evidence isnt enough overcome real inertia system time banking support group ran project Department Health encourage links GPs time banks make involvement patients delivery services reality experience nurses got stuck made difference doctors tended get bit stuck may fault true evidence isnt enough great deal evidence anecdotal otherwise best involve patients way makes possible pioneering work huge Voluntary Nursing Service New York VNSNY time banks Colchester Westminster Cheshire among frontline UK project VNSNY recently handed time bank 3 000 members across New York City big Catholic charity help members means care go lot lot deeper familiar rushed visit time certainly seems arrived know personal human scale going tough time continues ignore resources patients represent mainstream radical effective idea book co written Sarah Bird time banks health suggests need time banks part multi agency preventive infrastructure local level suggest every public sector provider contractor asked plan help rebuild social networks plan encourage mutual support among users answer question designed encourage innovation one way would embed time bank service use time credits way contract time banking organisation nearby turned difficult kind supportive mutual infrastructure funded standalone service however much money saves mainstream services need put responsibility firmly services public service contractors explain fund time banks transform services fulfil obligations encourage mutual support'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:02:46.662151Z",
     "start_time": "2019-05-17T06:02:46.598221Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/DSi6+/Share/p0h4y/ipynb/python-code/nlp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Push to dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Anaconda3/envs/caps_env/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/envs/caps_env/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/Anaconda3/envs/caps_env/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "# Push to dictionary\n",
    "dictionary = gensim.corpora.Dictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:01:51.634993Z",
     "start_time": "2019-05-17T06:01:51.629467Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_text = \"I'd had US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\"\n",
    "# print(full_text)\n",
    "# print(\"\\n\")\n",
    "\n",
    "\n",
    "# test_nlp = nlp(full_text,tokenizer, stopword_list)\n",
    "# # print(test_nlp._expand_contractions(full_text))\n",
    "# # test_norm = test_nlp.normalize()\n",
    "# print(test_nlp.normalize())\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"us unveils world powerful supercomputer beat china us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T22:54:33.584103Z",
     "start_time": "2019-05-16T22:54:33.554243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us unveil world power supercomput beat china us unveil world power supercomput call summit beat previou recordhold china sunway taihulight peak perform trillion calcul per second twice fast sunway taihulight capabl trillion calcul per second summit server reportedli take size two tenni court'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mf_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "mf_stemmer(test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T22:55:07.769107Z",
     "start_time": "2019-05-16T22:55:07.702676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1:  , , stopwords , computer\n",
      "Test2:  Some important text\n",
      "Test3:  Some Accented text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My system keep crash ! his crashed yesterday , ours crash daily'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# -------------------------------------------------\n",
    "def mf_remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "test = mf_remove_stopwords(\"The, and, if are stopwords, computer is not\")\n",
    "print(\"Test1: \", test)\n",
    "\n",
    "# --------------------------------------\n",
    "# --------------------------------------\n",
    "def mf_strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "# Test function\n",
    "test = mf_strip_html_tags(\"<html><h2>Some important text</h2></html>\")\n",
    "print(\"Test2: \", test)\n",
    "\n",
    "# --------------------------------------\n",
    "# --------------------------------------\n",
    "def mf_remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "# Test funciton\n",
    "test= mf_remove_accented_chars('Sómě Áccěntěd těxt')\n",
    "print(\"Test3: \", test)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# -------------------------------------------------------\n",
    "def mf_remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "mf_remove_special_characters(\"Well this was fun! What do you think? 123#@!\",remove_digits=True)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# -----------------------\n",
    "def mf_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "mf_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# -----------------------\n",
    "def mf_lemmatize(text):\n",
    "    text = nlp1(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "mf_lemmatize(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T06:03:06.174537Z",
     "start_time": "2019-05-16T06:03:02.796Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "expand_contractions(\"Y'all can't expand contractions I'd think\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
